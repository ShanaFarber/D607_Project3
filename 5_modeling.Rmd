---
title: "Modeling"
author: "Keith Colella"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, message = FALSE}
library(tidyverse)
library(randomForest)
```

## Read in Data

We loud in our previously constructed data frames. The `jobs_long` dataframe contains all jobs listings and associated skills. It's in the "long" format, so it cotnains significant duplication. The `job_listing` dataframe contains the same information but without the skills. As a result, there is no duplication, and each row constitutes a single observation (i.e. a single job listing).

```{r}
jobs_long <- read_csv('output/job_listings_skills_long.csv')
job_listings <- read_csv('output/job_listings_enhanced.csv') %>%
  select(-job_quals, -job_description)
```

### Modeling 

We need to use the is.na() function to check for missing or invalid values in each variable individually

```{r}
sum(is.na(job_listings$salary_max))
sum(is.na(job_listings$years_exp))
```

Using mean imputation to replace the missing values with the mean value of the corresponding variable

```{r}
# Replace missing values in years_exp with the mean value
job_listings$years_exp[is.na(job_listings$years_exp)] <- mean(job_listings$years_exp, na.rm = TRUE)

# Replace missing values in salary_max with the mean value
job_listings$salary_max[is.na(job_listings$salary_max)] <- mean(job_listings$salary_max, na.rm = TRUE)
```

A scatter plot and linear regression line, along with box plots for categorical variables.

```{r}
ggplot(job_listings, aes(x = years_exp, y = salary_max)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Years of Experience", y = "Maximum Salary") +
  scale_y_continuous(label = scales::comma)

job_listings %>%
  filter(!is.na(highest_ed)) %>%
  ggplot(aes(highest_ed, salary_max)) + 
  geom_boxplot() +
  labs(x = "Level of Education", y = "Maximum Salary") +
  scale_y_continuous(label = scales::comma)

job_listings %>%
  ggplot(aes(continent, salary_max)) + 
  geom_boxplot() +
  labs(x = "Location", y = "Maximum Salary") +
  scale_y_continuous(label = scales::comma)
```

We then fit our model and review results.

```{r}
ols <- lm(salary_max ~ highest_ed + years_exp + continent, data = job_listings)
summary(ols)
```


The model suggests that the predicted salary for an individual with 0 years of experience, no postgraduate education, and working in a remote location in this data set is $83,354.9. Having a master's degree or PhD is associated with higher salaries than having only a bachelor's degree, with master's degree adding $6,684.1 and PhD adding $26,731.7 to the predicted salary. Each additional year of experience is associated with an increase of $2,121.1 in predicted salary. Working in the Americas, Asia, or Oceania regions is associated with higher salaries than working in Europe or in a remote location, with the highest salary predicted for individuals working in the Americas. However, the coefficient for Europe is only marginally significant with a p-value of 0.00443, and the coefficient for working in a remote location is only marginally significant with a p-value of 0.04688.

The adjusted R-squared value of 0.1097 indicates that the predictors explain only about 10.97% of the variation in salary_max, which is relatively low. The p-value of < 2.2e-16 for the F-statistic indicates that the overall model is statistically significant

Overall, the significance of the location variables appear questionable. Moreover, our $R^2$ is very low, indicating potential misspecification. Given the unclear relationship with location variables, we refit the model with only years of experience and education.

```{r}
ols <- lm(salary_max ~ highest_ed +  years_exp, data = job_listings)
summary(ols)
```

Our fit appears much better. We can test out the model with a few predictions.

```{r}
test <- data.frame(highest_ed = c('bachelor','master','phd'),
                   years_exp = c(5,10,15))

scales::comma(predict(ols, test))
```

## Random Forest

Dummy encoding

```{r}
jobs_skills_matrix <- jobs_long %>%
  pivot_wider(names_from = highest_ed, values_from = highest_ed,
              names_prefix = 'ed_', values_fill = 0, values_fn = length) %>%
  pivot_wider(names_from = continent, values_from = continent,
              names_prefix = 'loc_', values_fill = 0, values_fn = length) %>%
  pivot_wider(names_from = skills, values_from = skills,
              values_fill = 0, values_fn = length) %>%
  column_to_rownames(., var = 'job_id') %>%
  select_if(is.numeric) %>%
  select(salary_max,
         years_exp,
         starts_with('ed_'),
         starts_with('loc_'),
         which(colSums(., na.rm = TRUE) > 1000),
         -ed_NA,
         -loc_NA,
         -salary_min)
  

colnames(jobs_skills_matrix) <- str_replace_all(
  colnames(jobs_skills_matrix),' ', '_')

head(jobs_skills_matrix)
```

Baseline Fit

```{r}
rf <- randomForest(salary_max ~ .,
                   data = jobs_skills_matrix,
                   ntree = 1000,
                   na.action = na.omit,
                   importance = TRUE)
rf
```

Visualize (i) feature importance and (ii) squared error as trees increase.

```{r}
imp <- as.data.frame(importance(rf))
imp$Var.Names <- row.names(imp)

ggplot(imp, aes(Var.Names, `%IncMSE`)) +
  geom_segment(aes(x = Var.Names, xend = Var.Names, y = 0, yend = `%IncMSE`)) +
  geom_point(aes(size = IncNodePurity)) +
  coord_flip() +
  theme(legend.position = 'bottom',
        panel.border = element_blank())

data.frame(tree = 1:rf$ntree, rmse = sqrt(rf$mse)) %>%
  ggplot(aes(tree, rmse)) +
  geom_line()
```

Export to Salary Prediction App

```{r}
saveRDS(rf, 'salary_prediction/salary_rf.rds')
write.csv(jobs_skills_matrix, 'salary_prediction/jobs_skills_matrix.csv', row.names = FALSE)
```


## KNN Recommender

TBD

```{r}
jobs_distances <- as.matrix(dist(jobs_skills_matrix, method="euclidean"))
dim(jobs_distances)
jobs_distances[1:10,1:10]
```
Source
https://rpubs.com/ferranmt/80166

```{r}
knn <- function(i, distance_matrix, k = 5) {
  neighbors <- data.frame(dist = distance_matrix[i,])
  k_nearest_ids <- arrange(neighbors, dist) %>% 
    slice(1:(k+1)) %>% 
    rownames()
  return(k_nearest_ids)
}
```

Test

```{r}
test <- jobs_skills_matrix[100,]
new_matrix <- rbind(jobs_skills_matrix, test)
new_distances <- as.matrix(dist(new_matrix, method="euclidean"))
last_row_name <- rownames(tail(new_distances,1))
match_ids <- knn(last_row_name, new_distances, 5)
match_ids <- match_ids[match_ids != last_row_name]
match_ids

job_listings[job_listings$job_id %in% match_ids,]
```

Save matrix for use in recommender Shiny app.

```{r}
write.csv(jobs_skills_matrix, 'job_recommendations/jobs_skills_matrix.csv', row.names = TRUE)
```