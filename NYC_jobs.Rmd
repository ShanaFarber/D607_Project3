---
title: "NYC_tidy"
author: "Waheeb Algabri"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

load required libraries 

```{r}
library(tidyverse)
library(dplyr)
library(knitr)
library(tidytext)
library(stringr)
library(ggwordcloud)
```

 
Read the data

```{r}
NYC_Jobs_2 <- read_csv("data/NYC_Jobs-2.csv")
```


```{r}
head(NYC_Jobs_2)
```

```{r}
library(tidyverse)
# extract necessary columns
selected_data <- NYC_Jobs_2 %>%
  select(`Job ID`, `Business Title`, `Job Description`, `Salary Range From`, `Salary Range To`, `Work Location`)

```


This code will select the Job Description column from the selected_data data frame, tokenize the words in each description using unnest_tokens(), remove any non-alphanumeric characters, and remove stop words using the filter() function. This will give us a tidy data frame of all the words in our job descriptions that are not stop words. From here, we can perform further analysis on these words to identify the most common data science skills.


```{r}
# Load required libraries
library(tidytext)
library(dplyr)
library(stringr)

# Clean and tokenize job descriptions
selected_data %>%
  select(`Job Description`) %>%
  unnest_tokens(word,`Job Description`) %>%
  mutate(word = str_replace_all(word, "[^[:alnum:]']", "")) %>%
  filter(!word %in% stop_words$word)

```

```{r}
# extract salary information
selected_data$MinSalaries <- as.numeric(str_extract(selected_data$`Salary Range From`, "\\d+"))
selected_data$MaxSalaries <- as.numeric(str_extract(selected_data$`Salary Range To`, "\\d+"))
selected_data$MeanSalary <- rowMeans(selected_data[,c("MinSalaries", "MaxSalaries")], na.rm = TRUE)

# filter out rows with missing salary information
selected_data <- selected_data %>%
  filter(!is.na(MinSalaries) & !is.na(MaxSalaries))
  
# see ranges of mean salaries
range(selected_data$MeanSalary)

```


```{r}
# subset data for Upper Echelon salaries
UpperEchelon <- selected_data %>% filter(selected_data$MeanSalary > 200)

```


```{r}
# tokenize and clean job descriptions for Upper Echelon salaries
description <- data.frame(UpperEchelon$`Job Description`)
desclist <- unlist(description)
desclist <- str_remove_all(desclist, '[[:punct:]]')
desclist <- str_remove_all(desclist, '[\r\n]')
desclist <- str_remove_all(desclist, '[0-9]')
desclist <- str_remove_all(desclist, '[\r\n]')
desclist <- tolower(desclist)
descsplit <- strsplit(desclist, " ")
```



```{r}
# count word frequencies
frequenciesUE <- table(unlist(descsplit))
frequenciesUE <- data.frame(frequenciesUE)
colnames(frequenciesUE) <- c('word', 'count')

```


```{r}
# omit frequently occurring words
omit <- c(" ", "and", "with", "from", "for", "the", "our", "your", "are", "will", "with", "that", "other", "all", "have", "to", "of", "", "this", "you", "a", "in", "is", "or", "as", "on", "be", "we", "by", "at", "an", "their", "us", "it", "can", "who", "such", "through", "into", "including", "one", "two", "three", "four", "five", "six", "not")
frequenciesUE <- subset(frequenciesUE, as.numeric(count) >= 3)
frequenciesUE_relevant <- frequenciesUE[!frequenciesUE$word %in% omit, ]
```


```{r}
# sort words by frequency in descending order
frequenciesUE_relevant <- frequenciesUE_relevant[order(-frequenciesUE_relevant$count), ]

```


```{r}
# plot top 50 frequent words
overweighted <- c("data", "analyst", "analysis", "analytics")
freqplot <- frequenciesUE_relevant[!frequenciesUE_relevant$word %in% overweighted, ]
freqplot <- freqplot[1:50, ]
ggplot(freqplot, aes(label = word, size = count, color = factor(sample.int(8, nrow(freqplot), replace = TRUE))))+
  geom_text_wordcloud()+
  scale_radius(range = c(0, 12), limits = c(0, NA)) +
  theme_minimal()



```


```{r}
# tokenize and clean job descriptions for salaries less than or equal to US mean
description <- data.frame(selected_data %>% filter(MeanSalary <= mean(selected_data$MeanSalary)) %>% select(`Job Description`))
desclist <- unlist(description)
desclist <- str_remove_all(desclist, '[[:punct:]]')
desclist <- str_remove_all(desclist, '[\r\n]')
desclist <- str_remove_all(desclist, '[0-9]')
desclist <- str_remove_all(desclist, '[\r\n]')
desclist <- tolower(desclist)
descsplit <- strsplit(desclist, " ")

# count word frequencies
frequenciesUSMean <- table(unlist(descsplit))
frequenciesUSMean <- data.frame(frequenciesUSMean)
colnames(frequenciesUSMean) <- c('word', 'count')

# omit infrequently occurring words
frequenciesUSMean <- subset(frequenciesUSMean, as.numeric(count) >= 3)

# omit stop words
omit <- c(" ", "and", "with", "from", "for", "the", "our", "your", "are", "will", "with", "that", "other", "all", "have", "to", "of", "", "this", "you", "a", "in", "is", "or", "as", "on", "be", "we", "by", "at", "an", "their", "us", "it", "can", "who", "such", "through", "into", "including", "one", "two", "three", "four", "five", "six", "not")
frequenciesUSMean_relevant <- frequenciesUSMean[!frequenciesUSMean$word %in% omit, ]

```


```{r}
# plot top 50 frequent words
overweighted <- c("data", "analyst", "analysis", "analytics")
freqplot <- frequenciesUSMean_relevant[!frequenciesUSMean_relevant$word %in% overweighted, ]
freqplot <- freqplot[1:50, ]
ggplot(freqplot, aes(label = word, size = count, color = factor(sample.int(8, nrow(freqplot), replace = TRUE))))+
  geom_text_wordcloud()+
  scale_radius(range = c(0, 12), limits = c(0, NA)) +
  theme_minimal()

```
